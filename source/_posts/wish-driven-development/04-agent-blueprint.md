## 回顾
经过前面三篇文章，我们其实已经实现了手动版的许愿驱动开发，回顾一下核心逻辑。我们发现绝大多数程序员有意愿做的就是简单描述一下自己的需求，然后期望得到代码。而这样的描述与许愿无异，LLM就像是一个灯神，可以实现我们的愿望。但是就跟很多玄幻小说里写的一样，要小心你许的愿望，因为不够精确的愿望会被扭曲的实现。抛开神秘主义的惊悚情节，这背后揭示出的一个道理是，我们提的需求本身是存在歧义的，它必须要在具体的上下文下才能精准的表达我们想要表达的含义。

![wdd-context-as-key-point](https://jtong-pic.obs.cn-north-4.myhuaweicloud.com/wish-driven-development/04-agent-blueprint/01-wdd-context-as-key-point.png)

不单是前后端这种上下文的差异，技术栈的差异甚至领域模型的差异都是上下文的一部分，好在所有这些都可以通过代码来表达。比如我们并不需要告诉大模型领域模型是什么，只要把表结构和领域模型类的代码给它，技术栈差异可能可以通过pom.xml、package.json，甚至你给他一个已经写过的代码比如一个用SpringBoot写的Controller，LLM自然知道在用什么，这一切就跟人通过这些信息了解上下文一样自然，我们可以通过什么理解或推理出上下文，LLM也可以。

所以才有了我们前面的那些工具，而这一切只有两个关键点：
	- 要许出粒度合适的愿望
	- 要选出足够的上下文
前者需要的是人能许出小步的愿望，后者则需要的是识别哪些文件或其他资源加载入我们的上下文。但是后者未必只能人来做。

![02-human-or-llm-generate-related-files](https://jtong-pic.obs.cn-north-4.myhuaweicloud.com/wish-driven-development/04-agent-blueprint/02-human-or-llm-generate-related-files.png)

既然我们的核心是找到合适的相关文件，那其实也可以考虑用大模型来找。尽管这中间可能有些浪费，幻觉目前还有些严重，但却是有刚需的，因为项目上来的新人可能不足以找到合适的上下文，且随着项目越来越大也不见得自己人就能找到完整的上下文，还是希望有一个助手能帮自己，而这种助手就适合用Agent来完成。

## 实现思路

我目前的做法是模拟人做开发的过程，分三步走：
- 第一步是先给完整的项目文件树，然后从中初步分析要改哪些文件。我们人实际上也是这样的对吧，基本上我们拿到一个需求分析完准备实现的时候，也是去文件树里找相关文件。不过我们可能还有一个概念性的架构图在脑子里，然后再映射到文件，目前我没有做这一步，因为每增加一步，幻觉就严重一点。
- 第二步是根据这些文件，把内容加载进Prompt来看是不是与愿望相关的。我们人也是这样的，在IDE里也是打开看看，当然我们比较熟了之后就不做这一步了，其实相当于我们用脑子对自己做了一些微调，还预加载了一些知识。当然这个过程要多循环几遍，根据我的经验，它幻觉较为严重还是比较容易出现漏的问题。
- 第三步是进行收敛，也就是那所有的这些已经找出来的内容，问它最小集是什么，让它帮忙过滤掉一些无关的内容。这个倒跟人不一样了，只是因为目前幻觉还是严重，不得已采用这个手段，目测这个手段还是有点用。

大概流程如下图：

![03-agent-blueprint](https://jtong-pic.obs.cn-north-4.myhuaweicloud.com/wish-driven-development/04-agent-blueprint/03-agent-blueprint.png)

### 初步分析的提示词

```markdown
## 技术上下文

我们在开发一个 vscode 插件，其工程的文件夹树形结构如下：

\```
{{ folder_tree }}
\```

## 相关文件

<在这里填入相关文件内容作为上下文>

## 输出格式

<此处省略>

## 最终目标任务

<%- target_task %>

## 任务

“最终目标任务”想要实现，需要从文件夹树形结构中的文件中读取的文件内容作为“最终目标任务”的上下文。

请问，你为了实现最终目标任务，你认为需要把哪些文件的内容放入提示词给你，你在思考“最终目标任务”时才没有上下文的缺失？将你的判断严格按上面的xml格式输出。
```

可以看出来，最终目标任务就是用户许的愿望，而我们期望大模型干的事情就是为最终目标任务的实现给出相关文件，以我们期望的格式（格式的部分我省略掉了，我们后面讲）。

### 相关性分析的提示词：

```markdown
## 技术上下文

我们在开发一个 vscode 插件，其工程的文件夹树形结构如下：

\```
{{ folder_tree }}
\```

## 相关文件

<在这里填入相关文件内容作为上下文>

{{#related_files_from }}
\```yaml
- path: import_read/index.js
  reader: all
  reason: "用于提取Java文件的import声明"
- path: llm_templates/development/step_1_xml-new-feature_detect_from_folder_tree.md
  reader: all
  reason: "用于生成最终目标任务中的技术上下文"
- path: openai_agent/index.js
  reader: all
  reason: "用于访问openai llm的agent"
- path: related_files_finder/index.js
  reader: all
  reason: "用于查找相关的文件"
- path: related_java_class_analysis/index.js
  reader: all
  reason: "用于提取Java类和方法的依赖"
- path: resolve_java_class_full_name/index.js
  reader: all
  reason: "用于解析Java类的完整名称"
- path: test/index.js
  reader: all
  reason: "用于测试prompt_render.js的渲染功能"
\```  
{{/related_files_from }}

## 输出格式

<此处省略>

## 最终目标任务

<%-target_task%>

## 任务

“最终目标任务”想要实现，需要从文件夹树形结构中的文件中读取的文件内容作为“最终目标任务”的上下文。
请根据已经在“相关文件”中的文件及其内容（其中三级标题是路径），判断是否提供了足够的上下文以写出“最终目标任务”中的代码，如果不够，你认为还可能需要的哪些文件树里的文件加载进来看看，严格按上面的xml格式输出额外需要的具体的文件。
```

这里呢，你可以看出，也是差不多的思路。这里可以看到我们把上一个提示词的输出放在了这里，我还让LLM给出了选择的理由，这也是最大限度利用人的智商进行过滤。

### 输出格式部分

我们用大模型生成，怎么能更好的截取代码呢？其中一种手法就是给出输出格式的要求，但是我们不知道大模型能不能完全输出完，万一截断了呢？所以我们可以这样写对输出格式的要求：

```markdown

## 输出格式

### yaml数据格式
\```yaml
- path: <文件路径>
  reader: <文件的reader>
  reason: "<说明这个文件为什么重要>"
\```

1. **路径 (`path`):** 表示文件的完整路径。例如，`src/main/java/dev/jtong/training/demo/smart/domain/controllers/UsersController.java` 定位到项目中的一个特定的Java文件。
  
2. **读取器 (`reader`):** 表示读取文件的读取器，默认是all。
    
3. **原因 (`reason`):** 描述为什么这个文件与实现功能相关。这可以包括文件的用途，比如是用于定义API端点、包含数据模型、数据库迁移脚本等。

### 支持的reader
#### controller
用于读取Java代码中的controller，会保留所有的field，可以针对性的要求读取某个函数，会保留函数的所有注解。值为`controller`。

#### model
用于读取Java代码中的 model，会保留所有的field，删除掉所有的setter和getter代码。值为`model`。

#### all
读取文件所有内容，不做任何静态分析和预处理。值为`all`。

### 外部xml格式

\```xml
<data>
<![CDATA[
- path: <文件路径>
  reader: <文件的reader>
  reason: "<说明这个文件为什么重要>"
]]> 
</data>
\```
我们在外部使用 XML 封装和传输 YAML 数据。这个格式将使用 CDATA 部分来确保 YAML 数据的空格、缩进和其他格式特征被完整地保留。
```

其实很简单，我们只要用一个xml标签把相关的内容都框起来就好了，因为有一个xml起始标签，那么其他的多余内容都可以截掉。而且也不怕截断问题，如果标签不闭合说明生成的有问题，让他重新生成一遍就好了。给出这个格式之后，我们还在命令的最后写了“严格按上面的xml格式输出额外需要的具体的文件。”，这也是很重要的，相当于加强了这个输出格式的权重。
有了这个技巧，我们就可以做到在不对模型进行微调的情况下最大限度的利用Chat模型来写agent了。

### 关于收敛

收敛部分其实提示词不难写，这里我就不给示例了，因为不管怎么做效果都不太好。这里面的问题在于幻觉，前面两步通常LLM都会给出过多的文本，你问的越多，他给的相关文件越多，等于要验证的内容越来越多，这也是个问题。

我现在的收敛做法只是简单让他给出“最小集”，内容多了也就是分批次让他给，但是这就意味着我们很浪费token，换句话说，成本会变高。如果人在过程中如果能做决定上下文的工作，就可以省掉这部分成本，而从另一个角度讲，我们更需要大上下文窗口了。

当然我们也在考虑可以不可以建一个小模型就做相关性分析，比如搞一个mixtral 7x8b专门做这个分析，这个可行性还不确定。

实际上我们可以全局来看，第一次从文件树得到初步的prompt本质上也是一次收敛，而那个判断大都不准，所以有的第二步在第一步的结果上发散，最后才有第三步的收敛。然而无论怎么做，这个领域呢，属于LLM不擅长的“做判断”的领域，LLM擅长的是“生成”而不是“做判断”。所以幻觉这个事情还是非常让人难受，不过换个角度想，这也是各个做大模型提效的平台部门的竞争力的关键，像我前面做的那些工具，根本没有啥必要，我一个人业余花两天都能做出来的，哪个一线团队不能自己搞呢？时代变了，大人。

## 代码怎么写

既然有思路了，那么接下来就该落代码了。很多人会以为我接下来要用langchain了是吧，然而这就是我们后面要讲的内容，因为我们一贯的风格都是所有代码都让LLM来写，而langchain是这个风格的巨大阻力。其实说得更过分一点，我是觉得langchain这类工具，是上一个时代的思维惯性的产物，它承载的各种实现有学习的意义，但是真引入项目中，对于AI提效非但没有好处，反而有坏处，所以我们选择了其他的实现策略。
